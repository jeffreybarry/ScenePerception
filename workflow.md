# ScenePerception Benchmark Suite: Workflow Guide

## Overview
The ScenePerception Benchmark Suite evaluates AI image generation models across five categories of foundational perception. This guide outlines the process for generating outputs, scoring, and reporting results.

## Process Workflow

### Step 1: Define Prompts
- Refer to `PROMPTS.md` for a complete list of prompts.
- Prompts are categorized by area: Text Rendering, Compositional Coherence, Semantic Grounding, Character Consistency, Realism vs. Stylization.

### Step 2: Generate Outputs
- Use approved AI image generators (e.g., Midjourney, DALLÂ·E).
- Record output metadata and save files in the `/images` directory.

### Step 3: Evaluation & Scoring
- Evaluate each image according to criteria outlined in `EVALUATION_GUIDE.md`.
- Record scores and observations in a structured format.

### Step 4: Reporting & Comparison
- Submit results to `RESULTS.md` or `/results` directory.
- Compare outputs by model type, prompt category, and evaluation scores.

## Contributing
See `CONTRIBUTING.md` for guidelines on submitting new prompts, evaluation criteria, or benchmarks.

